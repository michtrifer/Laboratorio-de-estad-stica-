{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d6f4ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f7b300",
   "metadata": {},
   "source": [
    "### Regresión simple (y = mpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "721e591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Motor Trend Car Road Tests.xlsx\")\n",
    "df = df.drop(columns=[\"model\"])\n",
    "X = df.drop(columns=[\"mpg\"])\n",
    "y = df[\"mpg\"]\n",
    "X = X.astype(float)\n",
    "y= y.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2c9ccc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.869</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.807</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   13.93</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 18 Sep 2025</td> <th>  Prob (F-statistic):</th> <td>3.79e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:13:00</td>     <th>  Log-Likelihood:    </th> <td> -69.855</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    32</td>      <th>  AIC:               </th> <td>   161.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    21</td>      <th>  BIC:               </th> <td>   177.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   12.3034</td> <td>   18.718</td> <td>    0.657</td> <td> 0.518</td> <td>  -26.623</td> <td>   51.229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cyl</th>   <td>   -0.1114</td> <td>    1.045</td> <td>   -0.107</td> <td> 0.916</td> <td>   -2.285</td> <td>    2.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>disp</th>  <td>    0.0133</td> <td>    0.018</td> <td>    0.747</td> <td> 0.463</td> <td>   -0.024</td> <td>    0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hp</th>    <td>   -0.0215</td> <td>    0.022</td> <td>   -0.987</td> <td> 0.335</td> <td>   -0.067</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>drat</th>  <td>    0.7871</td> <td>    1.635</td> <td>    0.481</td> <td> 0.635</td> <td>   -2.614</td> <td>    4.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wt</th>    <td>   -3.7153</td> <td>    1.894</td> <td>   -1.961</td> <td> 0.063</td> <td>   -7.655</td> <td>    0.224</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>qsec</th>  <td>    0.8210</td> <td>    0.731</td> <td>    1.123</td> <td> 0.274</td> <td>   -0.699</td> <td>    2.341</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>vs</th>    <td>    0.3178</td> <td>    2.105</td> <td>    0.151</td> <td> 0.881</td> <td>   -4.059</td> <td>    4.694</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>am</th>    <td>    2.5202</td> <td>    2.057</td> <td>    1.225</td> <td> 0.234</td> <td>   -1.757</td> <td>    6.797</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gear</th>  <td>    0.6554</td> <td>    1.493</td> <td>    0.439</td> <td> 0.665</td> <td>   -2.450</td> <td>    3.761</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>carb</th>  <td>   -0.1994</td> <td>    0.829</td> <td>   -0.241</td> <td> 0.812</td> <td>   -1.923</td> <td>    1.524</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.907</td> <th>  Durbin-Watson:     </th> <td>   1.861</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.385</td> <th>  Jarque-Bera (JB):  </th> <td>   1.747</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.521</td> <th>  Prob(JB):          </th> <td>   0.418</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.526</td> <th>  Cond. No.          </th> <td>1.22e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.22e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       mpg        & \\textbf{  R-squared:         } &     0.869   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.807   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     13.93   \\\\\n",
       "\\textbf{Date:}             & Thu, 18 Sep 2025 & \\textbf{  Prob (F-statistic):} &  3.79e-07   \\\\\n",
       "\\textbf{Time:}             &     22:13:00     & \\textbf{  Log-Likelihood:    } &   -69.855   \\\\\n",
       "\\textbf{No. Observations:} &          32      & \\textbf{  AIC:               } &     161.7   \\\\\n",
       "\\textbf{Df Residuals:}     &          21      & \\textbf{  BIC:               } &     177.8   \\\\\n",
       "\\textbf{Df Model:}         &          10      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &      12.3034  &       18.718     &     0.657  &         0.518        &      -26.623    &       51.229     \\\\\n",
       "\\textbf{cyl}   &      -0.1114  &        1.045     &    -0.107  &         0.916        &       -2.285    &        2.062     \\\\\n",
       "\\textbf{disp}  &       0.0133  &        0.018     &     0.747  &         0.463        &       -0.024    &        0.050     \\\\\n",
       "\\textbf{hp}    &      -0.0215  &        0.022     &    -0.987  &         0.335        &       -0.067    &        0.024     \\\\\n",
       "\\textbf{drat}  &       0.7871  &        1.635     &     0.481  &         0.635        &       -2.614    &        4.188     \\\\\n",
       "\\textbf{wt}    &      -3.7153  &        1.894     &    -1.961  &         0.063        &       -7.655    &        0.224     \\\\\n",
       "\\textbf{qsec}  &       0.8210  &        0.731     &     1.123  &         0.274        &       -0.699    &        2.341     \\\\\n",
       "\\textbf{vs}    &       0.3178  &        2.105     &     0.151  &         0.881        &       -4.059    &        4.694     \\\\\n",
       "\\textbf{am}    &       2.5202  &        2.057     &     1.225  &         0.234        &       -1.757    &        6.797     \\\\\n",
       "\\textbf{gear}  &       0.6554  &        1.493     &     0.439  &         0.665        &       -2.450    &        3.761     \\\\\n",
       "\\textbf{carb}  &      -0.1994  &        0.829     &    -0.241  &         0.812        &       -1.923    &        1.524     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  1.907 & \\textbf{  Durbin-Watson:     } &    1.861  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.385 & \\textbf{  Jarque-Bera (JB):  } &    1.747  \\\\\n",
       "\\textbf{Skew:}          &  0.521 & \\textbf{  Prob(JB):          } &    0.418  \\\\\n",
       "\\textbf{Kurtosis:}      &  2.526 & \\textbf{  Cond. No.          } & 1.22e+04  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.22e+04. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    mpg   R-squared:                       0.869\n",
       "Model:                            OLS   Adj. R-squared:                  0.807\n",
       "Method:                 Least Squares   F-statistic:                     13.93\n",
       "Date:                Thu, 18 Sep 2025   Prob (F-statistic):           3.79e-07\n",
       "Time:                        22:13:00   Log-Likelihood:                -69.855\n",
       "No. Observations:                  32   AIC:                             161.7\n",
       "Df Residuals:                      21   BIC:                             177.8\n",
       "Df Model:                          10                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         12.3034     18.718      0.657      0.518     -26.623      51.229\n",
       "cyl           -0.1114      1.045     -0.107      0.916      -2.285       2.062\n",
       "disp           0.0133      0.018      0.747      0.463      -0.024       0.050\n",
       "hp            -0.0215      0.022     -0.987      0.335      -0.067       0.024\n",
       "drat           0.7871      1.635      0.481      0.635      -2.614       4.188\n",
       "wt            -3.7153      1.894     -1.961      0.063      -7.655       0.224\n",
       "qsec           0.8210      0.731      1.123      0.274      -0.699       2.341\n",
       "vs             0.3178      2.105      0.151      0.881      -4.059       4.694\n",
       "am             2.5202      2.057      1.225      0.234      -1.757       6.797\n",
       "gear           0.6554      1.493      0.439      0.665      -2.450       3.761\n",
       "carb          -0.1994      0.829     -0.241      0.812      -1.923       1.524\n",
       "==============================================================================\n",
       "Omnibus:                        1.907   Durbin-Watson:                   1.861\n",
       "Prob(Omnibus):                  0.385   Jarque-Bera (JB):                1.747\n",
       "Skew:                           0.521   Prob(JB):                        0.418\n",
       "Kurtosis:                       2.526   Cond. No.                     1.22e+04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.22e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regresión \n",
    "X_ols=sm.add_constant(X)\n",
    "ols = sm.OLS(y, X_ols)\n",
    "results = ols.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cf83c8",
   "metadata": {},
   "source": [
    "const (12.30) → valor base del mpg cuando todas las demás variables son 0. \n",
    "\n",
    "cyl (-0.11) → más cilindros tienden a reducir un poco el rendimiento, aunque aquí el efecto es muy pequeño y no significativo.\n",
    "\n",
    "disp (+0.013) → mayor desplazamiento del motor se asocia con un ligero aumento del mpg, lo cual es raro (esperaríamos lo contrario). Esto pasa porque está correlacionado con otras variables.\n",
    "\n",
    "hp (-0.021) → más caballos de fuerza reducen el mpg, como se espera (motores más potentes consumen más).\n",
    "\n",
    "drat (+0.79) → una mayor relación de eje se asocia con más mpg, aunque no es claro en este modelo.\n",
    "\n",
    "wt (-3.72) → autos más pesados consumen más: cada 1000 libras extra reducen ~3.7 mpg. Este efecto sí es fuerte y consistente.\n",
    "\n",
    "qsec (+0.82) → un mayor tiempo en el cuarto de milla (autos más lentos en aceleración) se relaciona con mejor rendimiento. Tiene lógica: motores menos deportivos tienden a ser más eficientes.\n",
    "\n",
    "vs (+0.31) → motores en línea (vs=1) darían un poco más de mpg que los en V (vs=0), pero el efecto es muy débil aquí.\n",
    "\n",
    "am (+2.52) → transmisión manual tiende a dar más mpg que la automática (~2.5 mpg extra). \n",
    "\n",
    "gear (+0.65) → más marchas en la transmisión tienden a mejorar ligeramente el mpg.\n",
    "\n",
    "carb (-0.20) → más carburadores se asocian con menor eficiencia, aunque en este modelo el efecto es mínimo.\n",
    "\n",
    "El modelo explica alrededor del 87% de la variabilidad en el consumo (mpg). Como conjunto funciona muy bien para explicar el rendimiento de los autos, pero la mayoría de las variables están demasiado correlacionadas entre sí. El peso del coche es el predictor más importante y con más evidencia estadística de afectar al consumo, mientras que el resto no se distingue claramente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d74d627",
   "metadata": {},
   "source": [
    "### Train - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6c957b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest=train_test_split(X,y,train_size=.4,random_state=137)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15682647",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(Xtrain)\n",
    "Xtrain_scaled = scaler.transform(Xtrain)\n",
    "Xtest_scaled  = scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29b18157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michelle Trigo\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:1736: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=12\n",
      "  warnings.warn(\"kurtosistest only valid for n>=20 ... continuing \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.990</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.891</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   9.981</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 18 Sep 2025</td> <th>  Prob (F-statistic):</th>  <td> 0.242</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:06:51</td>     <th>  Log-Likelihood:    </th> <td> -9.9811</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    12</td>      <th>  AIC:               </th> <td>   41.96</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>     1</td>      <th>  BIC:               </th> <td>   47.30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   17.4333</td> <td>    0.556</td> <td>   31.361</td> <td> 0.020</td> <td>   10.370</td> <td>   24.497</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   40.7571</td> <td>   54.010</td> <td>    0.755</td> <td> 0.588</td> <td> -645.499</td> <td>  727.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -2.4635</td> <td>    3.021</td> <td>   -0.815</td> <td> 0.565</td> <td>  -40.854</td> <td>   35.927</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>  -10.1581</td> <td>   19.484</td> <td>   -0.521</td> <td> 0.694</td> <td> -257.727</td> <td>  237.411</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   14.1135</td> <td>   13.469</td> <td>    1.048</td> <td> 0.485</td> <td> -157.022</td> <td>  185.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   -0.8005</td> <td>    3.106</td> <td>   -0.258</td> <td> 0.839</td> <td>  -40.266</td> <td>   38.665</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    1.5727</td> <td>    3.475</td> <td>    0.453</td> <td> 0.729</td> <td>  -42.583</td> <td>   45.729</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>   32.4926</td> <td>   41.262</td> <td>    0.787</td> <td> 0.575</td> <td> -491.795</td> <td>  556.780</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>   36.1394</td> <td>   42.669</td> <td>    0.847</td> <td> 0.553</td> <td> -506.022</td> <td>  578.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>  -38.3551</td> <td>   43.625</td> <td>   -0.879</td> <td> 0.541</td> <td> -592.668</td> <td>  515.958</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>   11.9707</td> <td>   17.482</td> <td>    0.685</td> <td> 0.618</td> <td> -210.157</td> <td>  234.098</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.259</td> <th>  Durbin-Watson:     </th> <td>   2.354</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.879</td> <th>  Jarque-Bera (JB):  </th> <td>   0.413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.212</td> <th>  Prob(JB):          </th> <td>   0.813</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.196</td> <th>  Cond. No.          </th> <td>    382.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       mpg        & \\textbf{  R-squared:         } &     0.990   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.891   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     9.981   \\\\\n",
       "\\textbf{Date:}             & Thu, 18 Sep 2025 & \\textbf{  Prob (F-statistic):} &    0.242    \\\\\n",
       "\\textbf{Time:}             &     17:06:51     & \\textbf{  Log-Likelihood:    } &   -9.9811   \\\\\n",
       "\\textbf{No. Observations:} &          12      & \\textbf{  AIC:               } &     41.96   \\\\\n",
       "\\textbf{Df Residuals:}     &           1      & \\textbf{  BIC:               } &     47.30   \\\\\n",
       "\\textbf{Df Model:}         &          10      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &      17.4333  &        0.556     &    31.361  &         0.020        &       10.370    &       24.497     \\\\\n",
       "\\textbf{x1}    &      40.7571  &       54.010     &     0.755  &         0.588        &     -645.499    &      727.013     \\\\\n",
       "\\textbf{x2}    &      -2.4635  &        3.021     &    -0.815  &         0.565        &      -40.854    &       35.927     \\\\\n",
       "\\textbf{x3}    &     -10.1581  &       19.484     &    -0.521  &         0.694        &     -257.727    &      237.411     \\\\\n",
       "\\textbf{x4}    &      14.1135  &       13.469     &     1.048  &         0.485        &     -157.022    &      185.249     \\\\\n",
       "\\textbf{x5}    &      -0.8005  &        3.106     &    -0.258  &         0.839        &      -40.266    &       38.665     \\\\\n",
       "\\textbf{x6}    &       1.5727  &        3.475     &     0.453  &         0.729        &      -42.583    &       45.729     \\\\\n",
       "\\textbf{x7}    &      32.4926  &       41.262     &     0.787  &         0.575        &     -491.795    &      556.780     \\\\\n",
       "\\textbf{x8}    &      36.1394  &       42.669     &     0.847  &         0.553        &     -506.022    &      578.301     \\\\\n",
       "\\textbf{x9}    &     -38.3551  &       43.625     &    -0.879  &         0.541        &     -592.668    &      515.958     \\\\\n",
       "\\textbf{x10}   &      11.9707  &       17.482     &     0.685  &         0.618        &     -210.157    &      234.098     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  0.259 & \\textbf{  Durbin-Watson:     } &    2.354  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.879 & \\textbf{  Jarque-Bera (JB):  } &    0.413  \\\\\n",
       "\\textbf{Skew:}          &  0.212 & \\textbf{  Prob(JB):          } &    0.813  \\\\\n",
       "\\textbf{Kurtosis:}      &  2.196 & \\textbf{  Cond. No.          } &     382.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    mpg   R-squared:                       0.990\n",
       "Model:                            OLS   Adj. R-squared:                  0.891\n",
       "Method:                 Least Squares   F-statistic:                     9.981\n",
       "Date:                Thu, 18 Sep 2025   Prob (F-statistic):              0.242\n",
       "Time:                        17:06:51   Log-Likelihood:                -9.9811\n",
       "No. Observations:                  12   AIC:                             41.96\n",
       "Df Residuals:                       1   BIC:                             47.30\n",
       "Df Model:                          10                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         17.4333      0.556     31.361      0.020      10.370      24.497\n",
       "x1            40.7571     54.010      0.755      0.588    -645.499     727.013\n",
       "x2            -2.4635      3.021     -0.815      0.565     -40.854      35.927\n",
       "x3           -10.1581     19.484     -0.521      0.694    -257.727     237.411\n",
       "x4            14.1135     13.469      1.048      0.485    -157.022     185.249\n",
       "x5            -0.8005      3.106     -0.258      0.839     -40.266      38.665\n",
       "x6             1.5727      3.475      0.453      0.729     -42.583      45.729\n",
       "x7            32.4926     41.262      0.787      0.575    -491.795     556.780\n",
       "x8            36.1394     42.669      0.847      0.553    -506.022     578.301\n",
       "x9           -38.3551     43.625     -0.879      0.541    -592.668     515.958\n",
       "x10           11.9707     17.482      0.685      0.618    -210.157     234.098\n",
       "==============================================================================\n",
       "Omnibus:                        0.259   Durbin-Watson:                   2.354\n",
       "Prob(Omnibus):                  0.879   Jarque-Bera (JB):                0.413\n",
       "Skew:                           0.212   Prob(JB):                        0.813\n",
       "Kurtosis:                       2.196   Cond. No.                         382.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ols=sm.add_constant(Xtrain_scaled)\n",
    "ols = sm.OLS(ytrain, X_ols)\n",
    "results = ols.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0508e10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 train: 0.9900805114414537\n",
      "R2 test: -54.07178520418498\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = results.predict(sm.add_constant(Xtrain_scaled))\n",
    "y_pred_test  = results.predict(sm.add_constant(Xtest_scaled))\n",
    "\n",
    "print(\"R2 train:\", r2_score(ytrain, y_pred_train))\n",
    "print(\"R2 test:\",  r2_score(ytest, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49725d03",
   "metadata": {},
   "source": [
    "El modelo ajusta casi perfecto a los datos de entrenamiento (R2=0.99), pero al probarlo en datos nuevos su desempeño cae drásticamente (R2=-54), lo que indica un fuerte sobreajuste: con tan pocos datos de entrenamiento y muchas variables, la regresión prácticamente memorizó la muestra y no generaliza. En conclusión, aunque parece muy bueno en train, en realidad no sirve para predecir en test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024dc584",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5dbfeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercepto 17.433333333333334 Coeficientes [-1.91180995 -1.47680791  1.49298425  1.89807932 -0.66063204  1.45742956\n",
      " -0.05266481  1.49531023 -1.5340845  -0.82839949] R2 Train 0.9490586030413806 R2 Test 0.5420583567892368\n"
     ]
    }
   ],
   "source": [
    "# Lambda: .7\n",
    "ridge = Ridge(alpha=.7)  \n",
    "ridge.fit(Xtrain_scaled, ytrain)\n",
    "y_pred_ridge_train = ridge.predict(Xtrain_scaled)\n",
    "y_pred_ridge_test = ridge.predict(Xtest_scaled)\n",
    "\n",
    "\n",
    "r2_train = r2_score(ytrain, y_pred_ridge_train)\n",
    "r2_test = r2_score(ytest, y_pred_ridge_test)\n",
    "\n",
    "\n",
    "print(\"Intercepto\",ridge.intercept_,\"Coeficientes\",ridge.coef_, \"R2 Train\",r2_train,\"R2 Test\",r2_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7954d913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercepto 17.43333333333333 Coeficientes [-1.99332987 -1.69676335  1.98249196  2.12288228 -0.58317461  1.64679902\n",
      " -0.11551529  1.58200655 -1.84690932 -0.91193747] R2 Train 0.9579016413911189 R2 Test 0.48980134996965086\n"
     ]
    }
   ],
   "source": [
    "# Lambda: .5\n",
    "ridge = Ridge(alpha=.5)  \n",
    "ridge.fit(Xtrain_scaled, ytrain)\n",
    "y_pred_ridge_train = ridge.predict(Xtrain_scaled)\n",
    "y_pred_ridge_test = ridge.predict(Xtest_scaled)\n",
    "\n",
    "\n",
    "r2_train = r2_score(ytrain, y_pred_ridge_train)\n",
    "r2_test = r2_score(ytest, y_pred_ridge_test)\n",
    "\n",
    "\n",
    "print(\"Intercepto\",ridge.intercept_,\"Coeficientes\",ridge.coef_, \"R2 Train\",r2_train,\"R2 Test\",r2_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b120779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercepto 17.433333333333334 Coeficientes [-1.83753463 -1.33741463  1.14933154  1.73728638 -0.70486346  1.3219195\n",
      " -0.0027361   1.42934836 -1.30288694 -0.77612781] R2 Train 0.9418178078071817 R2 Test 0.5773542854640781\n"
     ]
    }
   ],
   "source": [
    "# Lambda: .9\n",
    "ridge = Ridge(alpha=.9)  \n",
    "ridge.fit(Xtrain_scaled, ytrain)\n",
    "y_pred_ridge_train = ridge.predict(Xtrain_scaled)\n",
    "y_pred_ridge_test = ridge.predict(Xtest_scaled)\n",
    "\n",
    "\n",
    "r2_train = r2_score(ytrain, y_pred_ridge_train)\n",
    "r2_test = r2_score(ytest, y_pred_ridge_test)\n",
    "\n",
    "\n",
    "print(\"Intercepto\",ridge.intercept_,\"Coeficientes\",ridge.coef_, \"R2 Train\",r2_train,\"R2 Test\",r2_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef101204",
   "metadata": {},
   "source": [
    "La regresión Ridge mejora la OLS normal porque, aunque ajusta un poco peor los datos de entrenamiento, logra predecir mucho mejor los datos nuevos, evitando el sobreajuste. El valor de lambda controla cuánto se penalizan los coeficientes; un lambda bajo deja los coeficientes grandes y ajusta muy bien en train pero mal en test, mientras que un lambda alto reduce los coeficientes, baja un poco el ajuste en train pero mejora la predicción en test. El objetivo es encontrar un lambda intermedio que logre un buen equilibrio entre ajustar los datos y generalizar a nuevos casos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff9b0ab",
   "metadata": {},
   "source": [
    "**___________________________________________________________________________________________________________________________**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68187877",
   "metadata": {},
   "source": [
    "### Regresión simple (y = qsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "660ddd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Motor Trend Car Road Tests.xlsx\")\n",
    "df = df.drop(columns=[\"model\"])\n",
    "X = df.drop(columns=[\"qsec\"])\n",
    "y = df[\"qsec\"]\n",
    "X = X.astype(float)\n",
    "y= y.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f7e73b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>qsec</td>       <th>  R-squared:         </th> <td>   0.875</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.815</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   14.66</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 18 Sep 2025</td> <th>  Prob (F-statistic):</th> <td>2.44e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:34:26</td>     <th>  Log-Likelihood:    </th> <td> -30.242</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    32</td>      <th>  AIC:               </th> <td>   82.48</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    21</td>      <th>  BIC:               </th> <td>   98.61</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   17.7762</td> <td>    3.876</td> <td>    4.586</td> <td> 0.000</td> <td>    9.716</td> <td>   25.837</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mpg</th>   <td>    0.0690</td> <td>    0.061</td> <td>    1.123</td> <td> 0.274</td> <td>   -0.059</td> <td>    0.197</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cyl</th>   <td>   -0.3627</td> <td>    0.293</td> <td>   -1.239</td> <td> 0.229</td> <td>   -0.971</td> <td>    0.246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>disp</th>  <td>   -0.0075</td> <td>    0.005</td> <td>   -1.505</td> <td> 0.147</td> <td>   -0.018</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hp</th>    <td>   -0.0016</td> <td>    0.006</td> <td>   -0.242</td> <td> 0.811</td> <td>   -0.015</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>drat</th>  <td>   -0.1311</td> <td>    0.476</td> <td>   -0.275</td> <td> 0.786</td> <td>   -1.121</td> <td>    0.859</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wt</th>    <td>    1.4963</td> <td>    0.500</td> <td>    2.990</td> <td> 0.007</td> <td>    0.456</td> <td>    2.537</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>vs</th>    <td>    0.9700</td> <td>    0.573</td> <td>    1.694</td> <td> 0.105</td> <td>   -0.221</td> <td>    2.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>am</th>    <td>   -0.9012</td> <td>    0.585</td> <td>   -1.540</td> <td> 0.139</td> <td>   -2.118</td> <td>    0.316</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gear</th>  <td>   -0.2013</td> <td>    0.433</td> <td>   -0.465</td> <td> 0.647</td> <td>   -1.101</td> <td>    0.699</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>carb</th>  <td>   -0.2736</td> <td>    0.233</td> <td>   -1.174</td> <td> 0.254</td> <td>   -0.758</td> <td>    0.211</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>21.069</td> <th>  Durbin-Watson:     </th> <td>   2.573</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  38.291</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.470</td> <th>  Prob(JB):          </th> <td>4.84e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 7.481</td> <th>  Cond. No.          </th> <td>8.77e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 8.77e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       qsec       & \\textbf{  R-squared:         } &     0.875   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.815   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     14.66   \\\\\n",
       "\\textbf{Date:}             & Thu, 18 Sep 2025 & \\textbf{  Prob (F-statistic):} &  2.44e-07   \\\\\n",
       "\\textbf{Time:}             &     22:34:26     & \\textbf{  Log-Likelihood:    } &   -30.242   \\\\\n",
       "\\textbf{No. Observations:} &          32      & \\textbf{  AIC:               } &     82.48   \\\\\n",
       "\\textbf{Df Residuals:}     &          21      & \\textbf{  BIC:               } &     98.61   \\\\\n",
       "\\textbf{Df Model:}         &          10      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &      17.7762  &        3.876     &     4.586  &         0.000        &        9.716    &       25.837     \\\\\n",
       "\\textbf{mpg}   &       0.0690  &        0.061     &     1.123  &         0.274        &       -0.059    &        0.197     \\\\\n",
       "\\textbf{cyl}   &      -0.3627  &        0.293     &    -1.239  &         0.229        &       -0.971    &        0.246     \\\\\n",
       "\\textbf{disp}  &      -0.0075  &        0.005     &    -1.505  &         0.147        &       -0.018    &        0.003     \\\\\n",
       "\\textbf{hp}    &      -0.0016  &        0.006     &    -0.242  &         0.811        &       -0.015    &        0.012     \\\\\n",
       "\\textbf{drat}  &      -0.1311  &        0.476     &    -0.275  &         0.786        &       -1.121    &        0.859     \\\\\n",
       "\\textbf{wt}    &       1.4963  &        0.500     &     2.990  &         0.007        &        0.456    &        2.537     \\\\\n",
       "\\textbf{vs}    &       0.9700  &        0.573     &     1.694  &         0.105        &       -0.221    &        2.161     \\\\\n",
       "\\textbf{am}    &      -0.9012  &        0.585     &    -1.540  &         0.139        &       -2.118    &        0.316     \\\\\n",
       "\\textbf{gear}  &      -0.2013  &        0.433     &    -0.465  &         0.647        &       -1.101    &        0.699     \\\\\n",
       "\\textbf{carb}  &      -0.2736  &        0.233     &    -1.174  &         0.254        &       -0.758    &        0.211     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 21.069 & \\textbf{  Durbin-Watson:     } &    2.573  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.000 & \\textbf{  Jarque-Bera (JB):  } &   38.291  \\\\\n",
       "\\textbf{Skew:}          &  1.470 & \\textbf{  Prob(JB):          } & 4.84e-09  \\\\\n",
       "\\textbf{Kurtosis:}      &  7.481 & \\textbf{  Cond. No.          } & 8.77e+03  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 8.77e+03. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   qsec   R-squared:                       0.875\n",
       "Model:                            OLS   Adj. R-squared:                  0.815\n",
       "Method:                 Least Squares   F-statistic:                     14.66\n",
       "Date:                Thu, 18 Sep 2025   Prob (F-statistic):           2.44e-07\n",
       "Time:                        22:34:26   Log-Likelihood:                -30.242\n",
       "No. Observations:                  32   AIC:                             82.48\n",
       "Df Residuals:                      21   BIC:                             98.61\n",
       "Df Model:                          10                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         17.7762      3.876      4.586      0.000       9.716      25.837\n",
       "mpg            0.0690      0.061      1.123      0.274      -0.059       0.197\n",
       "cyl           -0.3627      0.293     -1.239      0.229      -0.971       0.246\n",
       "disp          -0.0075      0.005     -1.505      0.147      -0.018       0.003\n",
       "hp            -0.0016      0.006     -0.242      0.811      -0.015       0.012\n",
       "drat          -0.1311      0.476     -0.275      0.786      -1.121       0.859\n",
       "wt             1.4963      0.500      2.990      0.007       0.456       2.537\n",
       "vs             0.9700      0.573      1.694      0.105      -0.221       2.161\n",
       "am            -0.9012      0.585     -1.540      0.139      -2.118       0.316\n",
       "gear          -0.2013      0.433     -0.465      0.647      -1.101       0.699\n",
       "carb          -0.2736      0.233     -1.174      0.254      -0.758       0.211\n",
       "==============================================================================\n",
       "Omnibus:                       21.069   Durbin-Watson:                   2.573\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               38.291\n",
       "Skew:                           1.470   Prob(JB):                     4.84e-09\n",
       "Kurtosis:                       7.481   Cond. No.                     8.77e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 8.77e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regresión \n",
    "X_ols=sm.add_constant(X)\n",
    "ols = sm.OLS(y, X_ols)\n",
    "results = ols.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faa4afe",
   "metadata": {},
   "source": [
    "El modelo explica alrededor del 87% de la variabilidad de qsec (tiempo en cuarto de milla).\n",
    "const (17.78) → valor base de qsec cuando todas las demás variables son cero.\n",
    "\n",
    "mpg (+0.069) → autos más eficientes tienden a tener un ligero aumento en qsec (autos más lentos en aceleración), aunque el efecto es muy pequeño y no significativo.\n",
    "\n",
    "cyl (-0.36) → más cilindros tienden a reducir qsec (autos más potentes aceleran más rápido), efecto pequeño.\n",
    "\n",
    "disp (-0.0075) → mayor desplazamiento del motor se asocia con un tiempo de cuarto de milla ligeramente menor (más rápido).\n",
    "\n",
    "hp (-0.0016) → más caballos de fuerza reducen un poco qsec (más rápido), efecto muy pequeño.\n",
    "\n",
    "drat (-0.13) → relación de eje mayor tiende a reducir ligeramente qsec, efecto débil.\n",
    "\n",
    "wt (+1.50) → autos más pesados tardan más en recorrer el cuarto de milla, efecto fuerte y significativo.\n",
    "\n",
    "vs (+0.97) → motor en línea (vs=1) aumenta qsec (autos más lentos), efecto moderado.\n",
    "\n",
    "am (-0.90) → transmisión manual tiende a hacer los autos más rápidos (reduce qsec).\n",
    "\n",
    "gear (-0.20) → más marchas tienden a reducir ligeramente qsec, efecto pequeño.\n",
    "\n",
    "carb (-0.27) → más carburadores tienden a reducir qsec (autos más deportivos), efecto pequeño."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913c25da",
   "metadata": {},
   "source": [
    "   ### Train - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44b88924",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest=train_test_split(X,y,train_size=.4,random_state=137)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9905761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(Xtrain)\n",
    "Xtrain_scaled = scaler.transform(Xtrain)\n",
    "Xtest_scaled  = scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f1c795f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michelle Trigo\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:1736: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=12\n",
      "  warnings.warn(\"kurtosistest only valid for n>=20 ... continuing \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>qsec</td>       <th>  R-squared:         </th> <td>   0.979</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.766</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4.609</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 18 Sep 2025</td> <th>  Prob (F-statistic):</th>  <td> 0.349</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:35:08</td>     <th>  Log-Likelihood:    </th> <td>-0.25847</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    12</td>      <th>  AIC:               </th> <td>   22.52</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>     1</td>      <th>  BIC:               </th> <td>   27.85</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   17.3733</td> <td>    0.247</td> <td>   70.269</td> <td> 0.009</td> <td>   14.232</td> <td>   20.515</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    1.0235</td> <td>    2.262</td> <td>    0.453</td> <td> 0.729</td> <td>  -27.713</td> <td>   29.760</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -0.3081</td> <td>   30.092</td> <td>   -0.010</td> <td> 0.993</td> <td> -382.663</td> <td>  382.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.2789</td> <td>    1.711</td> <td>    0.163</td> <td> 0.897</td> <td>  -21.465</td> <td>   22.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -1.0127</td> <td>    9.720</td> <td>   -0.104</td> <td> 0.934</td> <td> -124.519</td> <td>  122.494</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   -1.0356</td> <td>    8.615</td> <td>   -0.120</td> <td> 0.924</td> <td> -110.497</td> <td>  108.426</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    0.5976</td> <td>    1.295</td> <td>    0.461</td> <td> 0.725</td> <td>  -15.861</td> <td>   17.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>    0.2129</td> <td>   23.358</td> <td>    0.009</td> <td> 0.994</td> <td> -296.578</td> <td>  297.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>   -0.3877</td> <td>   24.867</td> <td>   -0.016</td> <td> 0.990</td> <td> -316.349</td> <td>  315.574</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>    0.6755</td> <td>   25.827</td> <td>    0.026</td> <td> 0.983</td> <td> -327.485</td> <td>  328.836</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>   -0.1562</td> <td>    9.422</td> <td>   -0.017</td> <td> 0.989</td> <td> -119.876</td> <td>  119.563</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>12.359</td> <th>  Durbin-Watson:     </th> <td>   2.150</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.002</td> <th>  Jarque-Bera (JB):  </th> <td>   7.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.532</td> <th>  Prob(JB):          </th> <td>  0.0293</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.178</td> <th>  Cond. No.          </th> <td>    517.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       qsec       & \\textbf{  R-squared:         } &     0.979   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.766   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     4.609   \\\\\n",
       "\\textbf{Date:}             & Thu, 18 Sep 2025 & \\textbf{  Prob (F-statistic):} &    0.349    \\\\\n",
       "\\textbf{Time:}             &     22:35:08     & \\textbf{  Log-Likelihood:    } &  -0.25847   \\\\\n",
       "\\textbf{No. Observations:} &          12      & \\textbf{  AIC:               } &     22.52   \\\\\n",
       "\\textbf{Df Residuals:}     &           1      & \\textbf{  BIC:               } &     27.85   \\\\\n",
       "\\textbf{Df Model:}         &          10      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &      17.3733  &        0.247     &    70.269  &         0.009        &       14.232    &       20.515     \\\\\n",
       "\\textbf{x1}    &       1.0235  &        2.262     &     0.453  &         0.729        &      -27.713    &       29.760     \\\\\n",
       "\\textbf{x2}    &      -0.3081  &       30.092     &    -0.010  &         0.993        &     -382.663    &      382.047     \\\\\n",
       "\\textbf{x3}    &       0.2789  &        1.711     &     0.163  &         0.897        &      -21.465    &       22.023     \\\\\n",
       "\\textbf{x4}    &      -1.0127  &        9.720     &    -0.104  &         0.934        &     -124.519    &      122.494     \\\\\n",
       "\\textbf{x5}    &      -1.0356  &        8.615     &    -0.120  &         0.924        &     -110.497    &      108.426     \\\\\n",
       "\\textbf{x6}    &       0.5976  &        1.295     &     0.461  &         0.725        &      -15.861    &       17.057     \\\\\n",
       "\\textbf{x7}    &       0.2129  &       23.358     &     0.009  &         0.994        &     -296.578    &      297.004     \\\\\n",
       "\\textbf{x8}    &      -0.3877  &       24.867     &    -0.016  &         0.990        &     -316.349    &      315.574     \\\\\n",
       "\\textbf{x9}    &       0.6755  &       25.827     &     0.026  &         0.983        &     -327.485    &      328.836     \\\\\n",
       "\\textbf{x10}   &      -0.1562  &        9.422     &    -0.017  &         0.989        &     -119.876    &      119.563     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 12.359 & \\textbf{  Durbin-Watson:     } &    2.150  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.002 & \\textbf{  Jarque-Bera (JB):  } &    7.063  \\\\\n",
       "\\textbf{Skew:}          &  1.532 & \\textbf{  Prob(JB):          } &   0.0293  \\\\\n",
       "\\textbf{Kurtosis:}      &  5.178 & \\textbf{  Cond. No.          } &     517.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   qsec   R-squared:                       0.979\n",
       "Model:                            OLS   Adj. R-squared:                  0.766\n",
       "Method:                 Least Squares   F-statistic:                     4.609\n",
       "Date:                Thu, 18 Sep 2025   Prob (F-statistic):              0.349\n",
       "Time:                        22:35:08   Log-Likelihood:               -0.25847\n",
       "No. Observations:                  12   AIC:                             22.52\n",
       "Df Residuals:                       1   BIC:                             27.85\n",
       "Df Model:                          10                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         17.3733      0.247     70.269      0.009      14.232      20.515\n",
       "x1             1.0235      2.262      0.453      0.729     -27.713      29.760\n",
       "x2            -0.3081     30.092     -0.010      0.993    -382.663     382.047\n",
       "x3             0.2789      1.711      0.163      0.897     -21.465      22.023\n",
       "x4            -1.0127      9.720     -0.104      0.934    -124.519     122.494\n",
       "x5            -1.0356      8.615     -0.120      0.924    -110.497     108.426\n",
       "x6             0.5976      1.295      0.461      0.725     -15.861      17.057\n",
       "x7             0.2129     23.358      0.009      0.994    -296.578     297.004\n",
       "x8            -0.3877     24.867     -0.016      0.990    -316.349     315.574\n",
       "x9             0.6755     25.827      0.026      0.983    -327.485     328.836\n",
       "x10           -0.1562      9.422     -0.017      0.989    -119.876     119.563\n",
       "==============================================================================\n",
       "Omnibus:                       12.359   Durbin-Watson:                   2.150\n",
       "Prob(Omnibus):                  0.002   Jarque-Bera (JB):                7.063\n",
       "Skew:                           1.532   Prob(JB):                       0.0293\n",
       "Kurtosis:                       5.178   Cond. No.                         517.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ols=sm.add_constant(Xtrain_scaled)\n",
    "ols = sm.OLS(ytrain, X_ols)\n",
    "results = ols.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c172f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 train: 0.978762192523052\n",
      "R2 test: 0.37610389619995843\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = results.predict(sm.add_constant(Xtrain_scaled))\n",
    "y_pred_test  = results.predict(sm.add_constant(Xtest_scaled))\n",
    "\n",
    "print(\"R2 train:\", r2_score(ytrain, y_pred_train))\n",
    "print(\"R2 test:\",  r2_score(ytest, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8934bc9",
   "metadata": {},
   "source": [
    "El modelo ajusta muy bien los datos de entrenamiento (R2 = 0.98), pero su desempeño en prueba cae mucho (R2 = 0.38), lo que indica sobreajuste con solo 12 datos de entrenamiento y muchas variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db34c1a",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bcb43fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercepto 17.433333333333334 Coeficientes [-1.91180995 -1.47680791  1.49298425  1.89807932 -0.66063204  1.45742956\n",
      " -0.05266481  1.49531023 -1.5340845  -0.82839949] R2 Train 0.9490586030413806 R2 Test 0.5420583567892368\n"
     ]
    }
   ],
   "source": [
    "# Lambda: .7\n",
    "ridge = Ridge(alpha=.7)  \n",
    "ridge.fit(Xtrain_scaled, ytrain)\n",
    "y_pred_ridge_train = ridge.predict(Xtrain_scaled)\n",
    "y_pred_ridge_test = ridge.predict(Xtest_scaled)\n",
    "\n",
    "\n",
    "r2_train = r2_score(ytrain, y_pred_ridge_train)\n",
    "r2_test = r2_score(ytest, y_pred_ridge_test)\n",
    "\n",
    "\n",
    "print(\"Intercepto\",ridge.intercept_,\"Coeficientes\",ridge.coef_, \"R2 Train\",r2_train,\"R2 Test\",r2_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f59872e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercepto 17.433333333333334 Coeficientes [-1.87361571 -1.39992258  1.30732255  1.81156932 -0.68552688  1.38448781\n",
      " -0.02635773  1.46043045 -1.41053881 -0.79954933] R2 Train 0.9452638793953312 R2 Test 0.5612475485912081\n"
     ]
    }
   ],
   "source": [
    "# Lambda: .8\n",
    "ridge = Ridge(alpha=.8)  \n",
    "ridge.fit(Xtrain_scaled, ytrain)\n",
    "y_pred_ridge_train = ridge.predict(Xtrain_scaled)\n",
    "y_pred_ridge_test = ridge.predict(Xtest_scaled)\n",
    "\n",
    "\n",
    "r2_train = r2_score(ytrain, y_pred_ridge_train)\n",
    "r2_test = r2_score(ytest, y_pred_ridge_test)\n",
    "\n",
    "\n",
    "print(\"Intercepto\",ridge.intercept_,\"Coeficientes\",ridge.coef_, \"R2 Train\",r2_train,\"R2 Test\",r2_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "915f4403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercepto 17.43333333333333 Coeficientes [-2.07112439 -2.90605012  4.04654138  3.04446081 -0.0214042   2.34685586\n",
      " -0.24193943  2.00723997 -3.10258427 -1.39667525] R2 Train 0.9805234796986707 R2 Test 0.23411656981043372\n"
     ]
    }
   ],
   "source": [
    "# Lambda: .1\n",
    "ridge = Ridge(alpha=.1)  \n",
    "ridge.fit(Xtrain_scaled, ytrain)\n",
    "y_pred_ridge_train = ridge.predict(Xtrain_scaled)\n",
    "y_pred_ridge_test = ridge.predict(Xtest_scaled)\n",
    "\n",
    "\n",
    "r2_train = r2_score(ytrain, y_pred_ridge_train)\n",
    "r2_test = r2_score(ytest, y_pred_ridge_test)\n",
    "\n",
    "\n",
    "print(\"Intercepto\",ridge.intercept_,\"Coeficientes\",ridge.coef_, \"R2 Train\",r2_train,\"R2 Test\",r2_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f90358",
   "metadata": {},
   "source": [
    "Lambda = 0.1 (muy baja) los coeficientes son grandes, el modelo ajusta casi perfectamente el entrenamiento (R2 = 0.98), pero falla mucho en test (R2 = 0.23). Esto indica sobreajuste.\n",
    "\n",
    "Lambda = 0.7 (intermedia) los coeficientes se reducen, el ajuste en train baja un poco (R2 = 0.95), pero la predicción en test mejora significativamente (R2 = 0.54). Aquí hay un mejor equilibrio entre ajustar y generalizar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae876bce",
   "metadata": {},
   "source": [
    "**_______________________________________________________________________________________________________________________**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bfc058",
   "metadata": {},
   "source": [
    "### Regresión dummies (y = mpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98ffa10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Motor Trend Car Road Tests.xlsx\")\n",
    "df = df.drop(columns=[\"model\"])\n",
    "X = df.drop(columns=[\"mpg\"])\n",
    "y = df[\"mpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe6f40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.get_dummies(X, columns=['cyl','gear','carb'])\n",
    "X = X.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb841041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01830887 -0.07862013  1.96436269 -2.60198229  0.38957261  2.02955372\n",
      "  0.64184981  0.00967623 -1.82438515  1.81470893 -1.72580159 -1.1309654\n",
      "  2.856767   -1.55689123 -2.61736311 -0.70136616 -0.57998694  0.54005481\n",
      "  4.91555263] 22.056737406563265 R2 0.8976755550941543\n"
     ]
    }
   ],
   "source": [
    "modelo=LinearRegression()\n",
    "modelo.fit(X,y)\n",
    "y_pred= modelo.predict(X)\n",
    "r2=r2_score(y,y_pred)\n",
    "print(modelo.coef_,modelo.intercept_, \"R2\",r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b7a46",
   "metadata": {},
   "source": [
    "Se hizo una regresión lineal para predecir el consumo de combustible (mpg) usando las características del auto y convirtiendo algunas variables en categorías (dummies). El modelo explica casi el 90% de la variación en mpg, lo que significa que ajusta bastante bien. El intercepto indica el valor base de mpg, y los coeficientes muestran cómo cada característica afecta el consumo: los positivos aumentan el mpg y los negativos lo reducen. En pocas palabras, el modelo captura bien cómo las distintas propiedades del auto influyen en su rendimiento de combustible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e7fef7",
   "metadata": {},
   "source": [
    "### Train - Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f576900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest=train_test_split(X,y,train_size=.4,random_state=137)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a213baf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 train: 1.0\n",
      "R2 test: 0.28303765950187\n"
     ]
    }
   ],
   "source": [
    "modelo_dummies = LinearRegression()\n",
    "modelo_dummies.fit(Xtrain, ytrain)\n",
    "y_pred_train = modelo_dummies.predict(Xtrain)\n",
    "y_pred_test = modelo_dummies.predict(Xtest)\n",
    "\n",
    "print(\"R2 train:\", r2_score(ytrain, y_pred_train))\n",
    "print(\"R2 test:\",  r2_score(ytest, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b42a6f",
   "metadata": {},
   "source": [
    "El R2 de entrenamiento es 1.0, lo que indica que el modelo ajusta perfectamente los datos de entrenamiento, el de prueba = 0.28 indica sobreajuste. Usar dummies con pocos datos de entrenamiento hace que el modelo se adapte demasiado a esos datos y pierda capacidad de predecir correctamente en nuevas observaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793e20bb",
   "metadata": {},
   "source": [
    "**_______________________________________________________________**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6acce7",
   "metadata": {},
   "source": [
    "### Regresión Dummies (y = qsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dfc7078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Motor Trend Car Road Tests.xlsx\")\n",
    "df = df.drop(columns=[\"model\"])\n",
    "X = df.drop(columns=[\"qsec\"])\n",
    "y = df[\"qsec\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b5e424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.get_dummies(X, columns=['cyl','gear','carb'])\n",
    "X = X.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eadf9acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01977242  0.00654616 -0.0022363  -0.12178602  0.41294793  0.31242369\n",
      " -1.63412266  1.56234142  0.22363696 -1.78597838 -0.4515632   0.98504616\n",
      " -0.53348296  0.65955613 -0.09818823  0.8623446  -1.00248583 -0.26679561\n",
      " -0.15443106] 16.31671033243791 R2 0.9057905221872551\n"
     ]
    }
   ],
   "source": [
    "modelo=LinearRegression()\n",
    "modelo.fit(X,y)\n",
    "y_pred= modelo.predict(X)\n",
    "r2=r2_score(y,y_pred)\n",
    "print(modelo.coef_,modelo.intercept_, \"R2\",r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b9e5eb",
   "metadata": {},
   "source": [
    "Logra un R2 de 0.91, lo que significa que explica aproximadamente el 90% de la variabilidad de qsec en los datos. Los coeficientes indican cómo cada variable afecta a qsec: valores positivos lo aumentan y valores negativos lo disminuyen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67112b1",
   "metadata": {},
   "source": [
    "### Train - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70952bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest=train_test_split(X,y,train_size=.4,random_state=137)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b22d6ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 train: 1.0\n",
      "R2 test: -0.22029889608534536\n"
     ]
    }
   ],
   "source": [
    "modelo_dummies = LinearRegression()\n",
    "modelo_dummies.fit(Xtrain, ytrain)\n",
    "y_pred_train = modelo_dummies.predict(Xtrain)\n",
    "y_pred_test = modelo_dummies.predict(Xtest)\n",
    "\n",
    "print(\"R2 train:\", r2_score(ytrain, y_pred_train))\n",
    "print(\"R2 test:\",  r2_score(ytest, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c7536",
   "metadata": {},
   "source": [
    "El modelo ajusta perfectamente los datos de entrenamiento (R2 = 1.0), pero no generaliza nada bien a los datos de prueba (R2 negativo, -0.22). Esto indica un sobreajuste extremo, el modelo memoriza los datos de entrenamiento en lugar de aprender patrones generales, por lo que falla completamente al predecir qsec en datos nuevos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7129e24d",
   "metadata": {},
   "source": [
    "### Comparación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f23d96",
   "metadata": {},
   "source": [
    "Para mpg, comparar el modelo sin dummies con el modelo con dummies muestra que incluir las variables categóricas transforma un poco mejor el ajuste: el R2 sube de aproximadamente 0.87–0.90 a 0.898. Esto significa que el modelo con dummies explica un poco más la variabilidad de mpg, aunque la mejora no es muy grande porque el dataset es pequeño y algunas categorías tienen pocas observaciones.\n",
    "\n",
    "Para qsec, la comparación es similar: el modelo sin dummies tiene un R2 de aproximadamente 0.875, mientras que con dummies sube a 0.906. Esto indica que agregar las variables categóricas ayuda a explicar mejor los tiempos de aceleración, aunque la diferencia sigue siendo moderada. En ambos casos, usar dummies mejora el modelo, pero no de manera dramática debido al tamaño y la estructura del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0fd5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
