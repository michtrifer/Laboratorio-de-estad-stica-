{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d6f4ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f7b300",
   "metadata": {},
   "source": [
    "### Regresi√≥n simple (y = mpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "721e591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Motor Trend Car Road Tests.xlsx\")\n",
    "df = df.drop(columns=[\"model\"])\n",
    "X = df.drop(columns=[\"mpg\"])\n",
    "y = df[\"mpg\"]\n",
    "X = X.astype(float)\n",
    "y= y.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2c9ccc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.869</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.807</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   13.93</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 18 Sep 2025</td> <th>  Prob (F-statistic):</th> <td>3.79e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:13:00</td>     <th>  Log-Likelihood:    </th> <td> -69.855</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    32</td>      <th>  AIC:               </th> <td>   161.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    21</td>      <th>  BIC:               </th> <td>   177.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   12.3034</td> <td>   18.718</td> <td>    0.657</td> <td> 0.518</td> <td>  -26.623</td> <td>   51.229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cyl</th>   <td>   -0.1114</td> <td>    1.045</td> <td>   -0.107</td> <td> 0.916</td> <td>   -2.285</td> <td>    2.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>disp</th>  <td>    0.0133</td> <td>    0.018</td> <td>    0.747</td> <td> 0.463</td> <td>   -0.024</td> <td>    0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hp</th>    <td>   -0.0215</td> <td>    0.022</td> <td>   -0.987</td> <td> 0.335</td> <td>   -0.067</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>drat</th>  <td>    0.7871</td> <td>    1.635</td> <td>    0.481</td> <td> 0.635</td> <td>   -2.614</td> <td>    4.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wt</th>    <td>   -3.7153</td> <td>    1.894</td> <td>   -1.961</td> <td> 0.063</td> <td>   -7.655</td> <td>    0.224</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>qsec</th>  <td>    0.8210</td> <td>    0.731</td> <td>    1.123</td> <td> 0.274</td> <td>   -0.699</td> <td>    2.341</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>vs</th>    <td>    0.3178</td> <td>    2.105</td> <td>    0.151</td> <td> 0.881</td> <td>   -4.059</td> <td>    4.694</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>am</th>    <td>    2.5202</td> <td>    2.057</td> <td>    1.225</td> <td> 0.234</td> <td>   -1.757</td> <td>    6.797</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gear</th>  <td>    0.6554</td> <td>    1.493</td> <td>    0.439</td> <td> 0.665</td> <td>   -2.450</td> <td>    3.761</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>carb</th>  <td>   -0.1994</td> <td>    0.829</td> <td>   -0.241</td> <td> 0.812</td> <td>   -1.923</td> <td>    1.524</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.907</td> <th>  Durbin-Watson:     </th> <td>   1.861</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.385</td> <th>  Jarque-Bera (JB):  </th> <td>   1.747</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.521</td> <th>  Prob(JB):          </th> <td>   0.418</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.526</td> <th>  Cond. No.          </th> <td>1.22e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.22e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       mpg        & \\textbf{  R-squared:         } &     0.869   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.807   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     13.93   \\\\\n",
       "\\textbf{Date:}             & Thu, 18 Sep 2025 & \\textbf{  Prob (F-statistic):} &  3.79e-07   \\\\\n",
       "\\textbf{Time:}             &     22:13:00     & \\textbf{  Log-Likelihood:    } &   -69.855   \\\\\n",
       "\\textbf{No. Observations:} &          32      & \\textbf{  AIC:               } &     161.7   \\\\\n",
       "\\textbf{Df Residuals:}     &          21      & \\textbf{  BIC:               } &     177.8   \\\\\n",
       "\\textbf{Df Model:}         &          10      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &      12.3034  &       18.718     &     0.657  &         0.518        &      -26.623    &       51.229     \\\\\n",
       "\\textbf{cyl}   &      -0.1114  &        1.045     &    -0.107  &         0.916        &       -2.285    &        2.062     \\\\\n",
       "\\textbf{disp}  &       0.0133  &        0.018     &     0.747  &         0.463        &       -0.024    &        0.050     \\\\\n",
       "\\textbf{hp}    &      -0.0215  &        0.022     &    -0.987  &         0.335        &       -0.067    &        0.024     \\\\\n",
       "\\textbf{drat}  &       0.7871  &        1.635     &     0.481  &         0.635        &       -2.614    &        4.188     \\\\\n",
       "\\textbf{wt}    &      -3.7153  &        1.894     &    -1.961  &         0.063        &       -7.655    &        0.224     \\\\\n",
       "\\textbf{qsec}  &       0.8210  &        0.731     &     1.123  &         0.274        &       -0.699    &        2.341     \\\\\n",
       "\\textbf{vs}    &       0.3178  &        2.105     &     0.151  &         0.881        &       -4.059    &        4.694     \\\\\n",
       "\\textbf{am}    &       2.5202  &        2.057     &     1.225  &         0.234        &       -1.757    &        6.797     \\\\\n",
       "\\textbf{gear}  &       0.6554  &        1.493     &     0.439  &         0.665        &       -2.450    &        3.761     \\\\\n",
       "\\textbf{carb}  &      -0.1994  &        0.829     &    -0.241  &         0.812        &       -1.923    &        1.524     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  1.907 & \\textbf{  Durbin-Watson:     } &    1.861  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.385 & \\textbf{  Jarque-Bera (JB):  } &    1.747  \\\\\n",
       "\\textbf{Skew:}          &  0.521 & \\textbf{  Prob(JB):          } &    0.418  \\\\\n",
       "\\textbf{Kurtosis:}      &  2.526 & \\textbf{  Cond. No.          } & 1.22e+04  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.22e+04. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    mpg   R-squared:                       0.869\n",
       "Model:                            OLS   Adj. R-squared:                  0.807\n",
       "Method:                 Least Squares   F-statistic:                     13.93\n",
       "Date:                Thu, 18 Sep 2025   Prob (F-statistic):           3.79e-07\n",
       "Time:                        22:13:00   Log-Likelihood:                -69.855\n",
       "No. Observations:                  32   AIC:                             161.7\n",
       "Df Residuals:                      21   BIC:                             177.8\n",
       "Df Model:                          10                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         12.3034     18.718      0.657      0.518     -26.623      51.229\n",
       "cyl           -0.1114      1.045     -0.107      0.916      -2.285       2.062\n",
       "disp           0.0133      0.018      0.747      0.463      -0.024       0.050\n",
       "hp            -0.0215      0.022     -0.987      0.335      -0.067       0.024\n",
       "drat           0.7871      1.635      0.481      0.635      -2.614       4.188\n",
       "wt            -3.7153      1.894     -1.961      0.063      -7.655       0.224\n",
       "qsec           0.8210      0.731      1.123      0.274      -0.699       2.341\n",
       "vs             0.3178      2.105      0.151      0.881      -4.059       4.694\n",
       "am             2.5202      2.057      1.225      0.234      -1.757       6.797\n",
       "gear           0.6554      1.493      0.439      0.665      -2.450       3.761\n",
       "carb          -0.1994      0.829     -0.241      0.812      -1.923       1.524\n",
       "==============================================================================\n",
       "Omnibus:                        1.907   Durbin-Watson:                   1.861\n",
       "Prob(Omnibus):                  0.385   Jarque-Bera (JB):                1.747\n",
       "Skew:                           0.521   Prob(JB):                        0.418\n",
       "Kurtosis:                       2.526   Cond. No.                     1.22e+04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.22e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regresi√≥n \n",
    "X_ols=sm.add_constant(X)\n",
    "ols = sm.OLS(y, X_ols)\n",
    "results = ols.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cf83c8",
   "metadata": {},
   "source": [
    "const (12.30) ‚Üí valor base del mpg cuando todas las dem√°s variables son 0. \n",
    "\n",
    "cyl (-0.11) ‚Üí m√°s cilindros tienden a reducir un poco el rendimiento, aunque aqu√≠ el efecto es muy peque√±o y no significativo.\n",
    "\n",
    "disp (+0.013) ‚Üí mayor desplazamiento del motor se asocia con un ligero aumento del mpg, lo cual es raro (esperar√≠amos lo contrario). Esto pasa porque est√° correlacionado con otras variables.\n",
    "\n",
    "hp (-0.021) ‚Üí m√°s caballos de fuerza reducen el mpg, como se espera (motores m√°s potentes consumen m√°s).\n",
    "\n",
    "drat (+0.79) ‚Üí una mayor relaci√≥n de eje se asocia con m√°s mpg, aunque no es claro en este modelo.\n",
    "\n",
    "wt (-3.72) ‚Üí autos m√°s pesados consumen m√°s: cada 1000 libras extra reducen ~3.7 mpg. Este efecto s√≠ es fuerte y consistente.\n",
    "\n",
    "qsec (+0.82) ‚Üí un mayor tiempo en el cuarto de milla (autos m√°s lentos en aceleraci√≥n) se relaciona con mejor rendimiento. Tiene l√≥gica: motores menos deportivos tienden a ser m√°s eficientes.\n",
    "\n",
    "vs (+0.31) ‚Üí motores en l√≠nea (vs=1) dar√≠an un poco m√°s de mpg que los en V (vs=0), pero el efecto es muy d√©bil aqu√≠.\n",
    "\n",
    "am (+2.52) ‚Üí transmisi√≥n manual tiende a dar m√°s mpg que la autom√°tica (~2.5 mpg extra). \n",
    "\n",
    "gear (+0.65) ‚Üí m√°s marchas en la transmisi√≥n tienden a mejorar ligeramente el mpg.\n",
    "\n",
    "carb (-0.20) ‚Üí m√°s carburadores se asocian con menor eficiencia, aunque en este modelo el efecto es m√≠nimo.\n",
    "\n",
    "El modelo explica alrededor del 87% de la variabilidad en el consumo (mpg). Como conjunto funciona muy bien para explicar el rendimiento de los autos, pero la mayor√≠a de las variables est√°n demasiado correlacionadas entre s√≠. El peso del coche es el predictor m√°s importante y con m√°s evidencia estad√≠stica de afectar al consumo, mientras que el resto no se distingue claramente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d74d627",
   "metadata": {},
   "source": [
    "### Train - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6c957b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest=train_test_split(X,y,train_size=.4,random_state=137)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15682647",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(Xtrain)\n",
    "Xtrain_scaled = scaler.transform(Xtrain)\n",
    "Xtest_scaled  = scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29b18157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michelle Trigo\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:1736: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=12\n",
      "  warnings.warn(\"kurtosistest only valid for n>=20 ... continuing \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.990</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.891</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   9.981</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 18 Sep 2025</td> <th>  Prob (F-statistic):</th>  <td> 0.242</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:06:51</td>     <th>  Log-Likelihood:    </th> <td> -9.9811</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    12</td>      <th>  AIC:               </th> <td>   41.96</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>     1</td>      <th>  BIC:               </th> <td>   47.30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   17.4333</td> <td>    0.556</td> <td>   31.361</td> <td> 0.020</td> <td>   10.370</td> <td>   24.497</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   40.7571</td> <td>   54.010</td> <td>    0.755</td> <td> 0.588</td> <td> -645.499</td> <td>  727.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -2.4635</td> <td>    3.021</td> <td>   -0.815</td> <td> 0.565</td> <td>  -40.854</td> <td>   35.927</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>  -10.1581</td> <td>   19.484</td> <td>   -0.521</td> <td> 0.694</td> <td> -257.727</td> <td>  237.411</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   14.1135</td> <td>   13.469</td> <td>    1.048</td> <td> 0.485</td> <td> -157.022</td> <td>  185.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   -0.8005</td> <td>    3.106</td> <td>   -0.258</td> <td> 0.839</td> <td>  -40.266</td> <td>   38.665</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    1.5727</td> <td>    3.475</td> <td>    0.453</td> <td> 0.729</td> <td>  -42.583</td> <td>   45.729</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>   32.4926</td> <td>   41.262</td> <td>    0.787</td> <td> 0.575</td> <td> -491.795</td> <td>  556.780</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>   36.1394</td> <td>   42.669</td> <td>    0.847</td> <td> 0.553</td> <td> -506.022</td> <td>  578.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>  -38.3551</td> <td>   43.625</td> <td>   -0.879</td> <td> 0.541</td> <td> -592.668</td> <td>  515.958</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>   11.9707</td> <td>   17.482</td> <td>    0.685</td> <td> 0.618</td> <td> -210.157</td> <td>  234.098</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.259</td> <th>  Durbin-Watson:     </th> <td>   2.354</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.879</td> <th>  Jarque-Bera (JB):  </th> <td>   0.413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.212</td> <th>  Prob(JB):          </th> <td>   0.813</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.196</td> <th>  Cond. No.          </th> <td>    382.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       mpg        & \\textbf{  R-squared:         } &     0.990   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.891   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     9.981   \\\\\n",
       "\\textbf{Date:}             & Thu, 18 Sep 2025 & \\textbf{  Prob (F-statistic):} &    0.242    \\\\\n",
       "\\textbf{Time:}             &     17:06:51     & \\textbf{  Log-Likelihood:    } &   -9.9811   \\\\\n",
       "\\textbf{No. Observations:} &          12      & \\textbf{  AIC:               } &     41.96   \\\\\n",
       "\\textbf{Df Residuals:}     &           1      & \\textbf{  BIC:               } &     47.30   \\\\\n",
       "\\textbf{Df Model:}         &          10      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &      17.4333  &        0.556     &    31.361  &         0.020        &       10.370    &       24.497     \\\\\n",
       "\\textbf{x1}    &      40.7571  &       54.010     &     0.755  &         0.588        &     -645.499    &      727.013     \\\\\n",
       "\\textbf{x2}    &      -2.4635  &        3.021     &    -0.815  &         0.565        &      -40.854    &       35.927     \\\\\n",
       "\\textbf{x3}    &     -10.1581  &       19.484     &    -0.521  &         0.694        &     -257.727    &      237.411     \\\\\n",
       "\\textbf{x4}    &      14.1135  &       13.469     &     1.048  &         0.485        &     -157.022    &      185.249     \\\\\n",
       "\\textbf{x5}    &      -0.8005  &        3.106     &    -0.258  &         0.839        &      -40.266    &       38.665     \\\\\n",
       "\\textbf{x6}    &       1.5727  &        3.475     &     0.453  &         0.729        &      -42.583    &       45.729     \\\\\n",
       "\\textbf{x7}    &      32.4926  &       41.262     &     0.787  &         0.575        &     -491.795    &      556.780     \\\\\n",
       "\\textbf{x8}    &      36.1394  &       42.669     &     0.847  &         0.553        &     -506.022    &      578.301     \\\\\n",
       "\\textbf{x9}    &     -38.3551  &       43.625     &    -0.879  &         0.541        &     -592.668    &      515.958     \\\\\n",
       "\\textbf{x10}   &      11.9707  &       17.482     &     0.685  &         0.618        &     -210.157    &      234.098     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  0.259 & \\textbf{  Durbin-Watson:     } &    2.354  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.879 & \\textbf{  Jarque-Bera (JB):  } &    0.413  \\\\\n",
       "\\textbf{Skew:}          &  0.212 & \\textbf{  Prob(JB):          } &    0.813  \\\\\n",
       "\\textbf{Kurtosis:}      &  2.196 & \\textbf{  Cond. No.          } &     382.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    mpg   R-squared:                       0.990\n",
       "Model:                            OLS   Adj. R-squared:                  0.891\n",
       "Method:                 Least Squares   F-statistic:                     9.981\n",
       "Date:                Thu, 18 Sep 2025   Prob (F-statistic):              0.242\n",
       "Time:                        17:06:51   Log-Likelihood:                -9.9811\n",
       "No. Observations:                  12   AIC:                             41.96\n",
       "Df Residuals:                       1   BIC:                             47.30\n",
       "Df Model:                          10                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         17.4333      0.556     31.361      0.020      10.370      24.497\n",
       "x1            40.7571     54.010      0.755      0.588    -645.499     727.013\n",
       "x2            -2.4635      3.021     -0.815      0.565     -40.854      35.927\n",
       "x3           -10.1581     19.484     -0.521      0.694    -257.727     237.411\n",
       "x4            14.1135     13.469      1.048      0.485    -157.022     185.249\n",
       "x5            -0.8005      3.106     -0.258      0.839     -40.266      38.665\n",
       "x6             1.5727      3.475      0.453      0.729     -42.583      45.729\n",
       "x7            32.4926     41.262      0.787      0.575    -491.795     556.780\n",
       "x8            36.1394     42.669      0.847      0.553    -506.022     578.301\n",
       "x9           -38.3551     43.625     -0.879      0.541    -592.668     515.958\n",
       "x10           11.9707     17.482      0.685      0.618    -210.157     234.098\n",
       "==============================================================================\n",
       "Omnibus:                        0.259   Durbin-Watson:                   2.354\n",
       "Prob(Omnibus):                  0.879   Jarque-Bera (JB):                0.413\n",
       "Skew:                           0.212   Prob(JB):                        0.813\n",
       "Kurtosis:                       2.196   Cond. No.                         382.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ols=sm.add_constant(Xtrain_scaled)\n",
    "ols = sm.OLS(ytrain, X_ols)\n",
    "results = ols.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0508e10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 train: 0.9900805114414537\n",
      "R2 test: -54.07178520418498\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = results.predict(sm.add_constant(Xtrain_scaled))\n",
    "y_pred_test  = results.predict(sm.add_constant(Xtest_scaled))\n",
    "\n",
    "print(\"R2 train:\", r2_score(ytrain, y_pred_train))\n",
    "print(\"R2 test:\",  r2_score(ytest, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49725d03",
   "metadata": {},
   "source": [
    "El modelo ajusta casi perfecto a los datos de entrenamiento (R2=0.99), pero al probarlo en datos nuevos su desempe√±o cae dr√°sticamente (R2=-54), lo que indica un fuerte sobreajuste: con tan pocos datos de entrenamiento y muchas variables, la regresi√≥n pr√°cticamente memoriz√≥ la muestra y no generaliza. En conclusi√≥n, aunque parece muy bueno en train, en realidad no sirve para predecir en test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024dc584",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5dbfeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercepto 17.433333333333334 Coeficientes [-1.91180995 -1.47680791  1.49298425  1.89807932 -0.66063204  1.45742956\n",
      " -0.05266481  1.49531023 -1.5340845  -0.82839949] R2 Train 0.9490586030413806 R2 Test 0.5420583567892368\n"
     ]
    }
   ],
   "source": [
    "# Lambda: .7\n",
    "ridge = Ridge(alpha=.7)  \n",
    "ridge.fit(Xtrain_scaled, ytrain)\n",
    "y_pred_ridge_train = ridge.predict(Xtrain_scaled)\n",
    "y_pred_ridge_test = ridge.predict(Xtest_scaled)\n",
    "\n",
    "\n",
    "r2_train = r2_score(ytrain, y_pred_ridge_train)\n",
    "r2_test = r2_score(ytest, y_pred_ridge_test)\n",
    "\n",
    "\n",
    "print(\"Intercepto\",ridge.intercept_,\"Coeficientes\",ridge.coef_, \"R2 Train\",r2_train,\"R2 Test\",r2_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7954d913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercepto 17.43333333333333 Coeficientes [-1.99332987 -1.69676335  1.98249196  2.12288228 -0.58317461  1.64679902\n",
      " -0.11551529  1.58200655 -1.84690932 -0.91193747] R2 Train 0.9579016413911189 R2 Test 0.48980134996965086\n"
     ]
    }
   ],
   "source": [
    "# Lambda: .5\n",
    "ridge = Ridge(alpha=.5)  \n",
    "ridge.fit(Xtrain_scaled, ytrain)\n",
    "y_pred_ridge_train = ridge.predict(Xtrain_scaled)\n",
    "y_pred_ridge_test = ridge.predict(Xtest_scaled)\n",
    "\n",
    "\n",
    "r2_train = r2_score(ytrain, y_pred_ridge_train)\n",
    "r2_test = r2_score(ytest, y_pred_ridge_test)\n",
    "\n",
    "\n",
    "print(\"Intercepto\",ridge.intercept_,\"Coeficientes\",ridge.coef_, \"R2 Train\",r2_train,\"R2 Test\",r2_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b120779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercepto 17.433333333333334 Coeficientes [-1.83753463 -1.33741463  1.14933154  1.73728638 -0.70486346  1.3219195\n",
      " -0.0027361   1.42934836 -1.30288694 -0.77612781] R2 Train 0.9418178078071817 R2 Test 0.5773542854640781\n"
     ]
    }
   ],
   "source": [
    "# Lambda: .9\n",
    "ridge = Ridge(alpha=.9)  \n",
    "ridge.fit(Xtrain_scaled, ytrain)\n",
    "y_pred_ridge_train = ridge.predict(Xtrain_scaled)\n",
    "y_pred_ridge_test = ridge.predict(Xtest_scaled)\n",
    "\n",
    "\n",
    "r2_train = r2_score(ytrain, y_pred_ridge_train)\n",
    "r2_test = r2_score(ytest, y_pred_ridge_test)\n",
    "\n",
    "\n",
    "print(\"Intercepto\",ridge.intercept_,\"Coeficientes\",ridge.coef_, \"R2 Train\",r2_train,\"R2 Test\",r2_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef101204",
   "metadata": {},
   "source": [
    "La regresi√≥n Ridge mejora la OLS normal porque, aunque ajusta un poco peor los datos de entrenamiento, logra predecir mucho mejor los datos nuevos, evitando el sobreajuste. El valor de lambda controla cu√°nto se penalizan los coeficientes; un lambda bajo deja los coeficientes grandes y ajusta muy bien en train pero mal en test, mientras que un lambda alto reduce los coeficientes, baja un poco el ajuste en train pero mejora la predicci√≥n en test. El objetivo es encontrar un lambda intermedio que logre un buen equilibrio entre ajustar los datos y generalizar a nuevos casos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff9b0ab",
   "metadata": {},
   "source": [
    "**___________________________________________________________________________________________________________________________**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68187877",
   "metadata": {},
   "source": [
    "### Regresi√≥n simple (y = qsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "660ddd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Motor Trend Car Road Tests.xlsx\")\n",
    "df = df.drop(columns=[\"model\"])\n",
    "X = df.drop(columns=[\"qsec\"])\n",
    "y = df[\"qsec\"]\n",
    "X = X.astype(float)\n",
    "y= y.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f7e73b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>qsec</td>       <th>  R-squared:         </th> <td>   0.875</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.815</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   14.66</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 18 Sep 2025</td> <th>  Prob (F-statistic):</th> <td>2.44e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:34:26</td>     <th>  Log-Likelihood:    </th> <td> -30.242</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    32</td>      <th>  AIC:               </th> <td>   82.48</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    21</td>      <th>  BIC:               </th> <td>   98.61</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   17.7762</td> <td>    3.876</td> <td>    4.586</td> <td> 0.000</td> <td>    9.716</td> <td>   25.837</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mpg</th>   <td>    0.0690</td> <td>    0.061</td> <td>    1.123</td> <td> 0.274</td> <td>   -0.059</td> <td>    0.197</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cyl</th>   <td>   -0.3627</td> <td>    0.293</td> <td>   -1.239</td> <td> 0.229</td> <td>   -0.971</td> <td>    0.246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>disp</th>  <td>   -0.0075</td> <td>    0.005</td> <td>   -1.505</td> <td> 0.147</td> <td>   -0.018</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hp</th>    <td>   -0.0016</td> <td>    0.006</td> <td>   -0.242</td> <td> 0.811</td> <td>   -0.015</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>drat</th>  <td>   -0.1311</td> <td>    0.476</td> <td>   -0.275</td> <td> 0.786</td> <td>   -1.121</td> <td>    0.859</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wt</th>    <td>    1.4963</td> <td>    0.500</td> <td>    2.990</td> <td> 0.007</td> <td>    0.456</td> <td>    2.537</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>vs</th>    <td>    0.9700</td> <td>    0.573</td> <td>    1.694</td> <td> 0.105</td> <td>   -0.221</td> <td>    2.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>am</th>    <td>   -0.9012</td> <td>    0.585</td> <td>   -1.540</td> <td> 0.139</td> <td>   -2.118</td> <td>    0.316</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gear</th>  <td>   -0.2013</td> <td>    0.433</td> <td>   -0.465</td> <td> 0.647</td> <td>   -1.101</td> <td>    0.699</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>carb</th>  <td>   -0.2736</td> <td>    0.233</td> <td>   -1.174</td> <td> 0.254</td> <td>   -0.758</td> <td>    0.211</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>21.069</td> <th>  Durbin-Watson:     </th> <td>   2.573</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  38.291</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.470</td> <th>  Prob(JB):          </th> <td>4.84e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 7.481</td> <th>  Cond. No.          </th> <td>8.77e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 8.77e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       qsec       & \\textbf{  R-squared:         } &     0.875   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.815   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     14.66   \\\\\n",
       "\\textbf{Date:}             & Thu, 18 Sep 2025 & \\textbf{  Prob (F-statistic):} &  2.44e-07   \\\\\n",
       "\\textbf{Time:}             &     22:34:26     & \\textbf{  Log-Likelihood:    } &   -30.242   \\\\\n",
       "\\textbf{No. Observations:} &          32      & \\textbf{  AIC:               } &     82.48   \\\\\n",
       "\\textbf{Df Residuals:}     &          21      & \\textbf{  BIC:               } &     98.61   \\\\\n",
       "\\textbf{Df Model:}         &          10      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &      17.7762  &        3.876     &     4.586  &         0.000        &        9.716    &       25.837     \\\\\n",
       "\\textbf{mpg}   &       0.0690  &        0.061     &     1.123  &         0.274        &       -0.059    &        0.197     \\\\\n",
       "\\textbf{cyl}   &      -0.3627  &        0.293     &    -1.239  &         0.229        &       -0.971    &        0.246     \\\\\n",
       "\\textbf{disp}  &      -0.0075  &        0.005     &    -1.505  &         0.147        &       -0.018    &        0.003     \\\\\n",
       "\\textbf{hp}    &      -0.0016  &        0.006     &    -0.242  &         0.811        &       -0.015    &        0.012     \\\\\n",
       "\\textbf{drat}  &      -0.1311  &        0.476     &    -0.275  &         0.786        &       -1.121    &        0.859     \\\\\n",
       "\\textbf{wt}    &       1.4963  &        0.500     &     2.990  &         0.007        &        0.456    &        2.537     \\\\\n",
       "\\textbf{vs}    &       0.9700  &        0.573     &     1.694  &         0.105        &       -0.221    &        2.161     \\\\\n",
       "\\textbf{am}    &      -0.9012  &        0.585     &    -1.540  &         0.139        &       -2.118    &        0.316     \\\\\n",
       "\\textbf{gear}  &      -0.2013  &        0.433     &    -0.465  &         0.647        &       -1.101    &        0.699     \\\\\n",
       "\\textbf{carb}  &      -0.2736  &        0.233     &    -1.174  &         0.254        &       -0.758    &        0.211     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 21.069 & \\textbf{  Durbin-Watson:     } &    2.573  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.000 & \\textbf{  Jarque-Bera (JB):  } &   38.291  \\\\\n",
       "\\textbf{Skew:}          &  1.470 & \\textbf{  Prob(JB):          } & 4.84e-09  \\\\\n",
       "\\textbf{Kurtosis:}      &  7.481 & \\textbf{  Cond. No.          } & 8.77e+03  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 8.77e+03. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   qsec   R-squared:                       0.875\n",
       "Model:                            OLS   Adj. R-squared:                  0.815\n",
       "Method:                 Least Squares   F-statistic:                     14.66\n",
       "Date:                Thu, 18 Sep 2025   Prob (F-statistic):           2.44e-07\n",
       "Time:                        22:34:26   Log-Likelihood:                -30.242\n",
       "No. Observations:                  32   AIC:                             82.48\n",
       "Df Residuals:                      21   BIC:                             98.61\n",
       "Df Model:                          10                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         17.7762      3.876      4.586      0.000       9.716      25.837\n",
       "mpg            0.0690      0.061      1.123      0.274      -0.059       0.197\n",
       "cyl           -0.3627      0.293     -1.239      0.229      -0.971       0.246\n",
       "disp          -0.0075      0.005     -1.505      0.147      -0.018       0.003\n",
       "hp            -0.0016      0.006     -0.242      0.811      -0.015       0.012\n",
       "drat          -0.1311      0.476     -0.275      0.786      -1.121       0.859\n",
       "wt             1.4963      0.500      2.990      0.007       0.456       2.537\n",
       "vs             0.9700      0.573      1.694      0.105      -0.221       2.161\n",
       "am            -0.9012      0.585     -1.540      0.139      -2.118       0.316\n",
       "gear          -0.2013      0.433     -0.465      0.647      -1.101       0.699\n",
       "carb          -0.2736      0.233     -1.174      0.254      -0.758       0.211\n",
       "==============================================================================\n",
       "Omnibus:                       21.069   Durbin-Watson:                   2.573\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               38.291\n",
       "Skew:                           1.470   Prob(JB):                     4.84e-09\n",
       "Kurtosis:                       7.481   Cond. No.                     8.77e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 8.77e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regresi√≥n \n",
    "X_ols=sm.add_constant(X)\n",
    "ols = sm.OLS(y, X_ols)\n",
    "results = ols.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faa4afe",
   "metadata": {},
   "source": [
    "El modelo explica alrededor del 87% de la variabilidad de qsec (tiempo en cuarto de milla).\n",
    "const (17.78) ‚Üí valor base de qsec cuando todas las dem√°s variables son cero.\n",
    "\n",
    "mpg (+0.069) ‚Üí autos m√°s eficientes tienden a tener un ligero aumento en qsec (autos m√°s lentos en aceleraci√≥n), aunque el efecto es muy peque√±o y no significativo.\n",
    "\n",
    "cyl (-0.36) ‚Üí m√°s cilindros tienden a reducir qsec (autos m√°s potentes aceleran m√°s r√°pido), efecto peque√±o.\n",
    "\n",
    "disp (-0.0075) ‚Üí mayor desplazamiento del motor se asocia con un tiempo de cuarto de milla ligeramente menor (m√°s r√°pido).\n",
    "\n",
    "hp (-0.0016) ‚Üí m√°s caballos de fuerza reducen un poco qsec (m√°s r√°pido), efecto muy peque√±o.\n",
    "\n",
    "drat (-0.13) ‚Üí relaci√≥n de eje mayor tiende a reducir ligeramente qsec, efecto d√©bil.\n",
    "\n",
    "wt (+1.50) ‚Üí autos m√°s pesados tardan m√°s en recorrer el cuarto de milla, efecto fuerte y significativo.\n",
    "\n",
    "vs (+0.97) ‚Üí motor en l√≠nea (vs=1) aumenta qsec (autos m√°s lentos), efecto moderado.\n",
    "\n",
    "am (-0.90) ‚Üí transmisi√≥n manual tiende a hacer los autos m√°s r√°pidos (reduce qsec).\n",
    "\n",
    "gear (-0.20) ‚Üí m√°s marchas tienden a reducir ligeramente qsec, efecto peque√±o.\n",
    "\n",
    "carb (-0.27) ‚Üí m√°s carburadores tienden a reducir qsec (autos m√°s deportivos), efecto peque√±o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913c25da",
   "metadata": {},
   "source": [
    "   ### Train - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44b88924",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest=train_test_split(X,y,train_size=.4,random_state=137)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9905761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(Xtrain)\n",
    "Xtrain_scaled = scaler.transform(Xtrain)\n",
    "Xtest_scaled  = scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f1c795f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michelle Trigo\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:1736: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=12\n",
      "  warnings.warn(\"kurtosistest only valid for n>=20 ... continuing \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>qsec</td>       <th>  R-squared:         </th> <td>   0.979</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.766</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4.609</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 18 Sep 2025</td> <th>  Prob (F-statistic):</th>  <td> 0.349</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:35:08</td>     <th>  Log-Likelihood:    </th> <td>-0.25847</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    12</td>      <th>  AIC:               </th> <td>   22.52</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>     1</td>      <th>  BIC:               </th> <td>   27.85</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   17.3733</td> <td>    0.247</td> <td>   70.269</td> <td> 0.009</td> <td>   14.232</td> <td>   20.515</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    1.0235</td> <td>    2.262</td> <td>    0.453</td> <td> 0.729</td> <td>  -27.713</td> <td>   29.760</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -0.3081</td> <td>   30.092</td> <td>   -0.010</td> <td> 0.993</td> <td> -382.663</td> <td>  382.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.2789</td> <td>    1.711</td> <td>    0.163</td> <td> 0.897</td> <td>  -21.465</td> <td>   22.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -1.0127</td> <td>    9.720</td> <td>   -0.104</td> <td> 0.934</td> <td> -124.519</td> <td>  122.494</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   -1.0356</td> <td>    8.615</td> <td>   -0.120</td> <td> 0.924</td> <td> -110.497</td> <td>  108.426</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    0.5976</td> <td>    1.295</td> <td>    0.461</td> <td> 0.725</td> <td>  -15.861</td> <td>   17.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>    0.2129</td> <td>   23.358</td> <td>    0.009</td> <td> 0.994</td> <td> -296.578</td> <td>  297.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>   -0.3877</td> <td>   24.867</td> <td>   -0.016</td> <td> 0.990</td> <td> -316.349</td> <td>  315.574</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>    0.6755</td> <td>   25.827</td> <td>    0.026</td> <td> 0.983</td> <td> -327.485</td> <td>  328.836</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>   -0.1562</td> <td>    9.422</td> <td>   -0.017</td> <td> 0.989</td> <td> -119.876</td> <td>  119.563</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>12.359</td> <th>  Durbin-Watson:     </th> <td>   2.150</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.002</td> <th>  Jarque-Bera (JB):  </th> <td>   7.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.532</td> <th>  Prob(JB):          </th> <td>  0.0293</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.178</td> <th>  Cond. No.          </th> <td>    517.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       qsec       & \\textbf{  R-squared:         } &     0.979   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.766   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     4.609   \\\\\n",
       "\\textbf{Date:}             & Thu, 18 Sep 2025 & \\textbf{  Prob (F-statistic):} &    0.349    \\\\\n",
       "\\textbf{Time:}             &     22:35:08     & \\textbf{  Log-Likelihood:    } &  -0.25847   \\\\\n",
       "\\textbf{No. Observations:} &          12      & \\textbf{  AIC:               } &     22.52   \\\\\n",
       "\\textbf{Df Residuals:}     &           1      & \\textbf{  BIC:               } &     27.85   \\\\\n",
       "\\textbf{Df Model:}         &          10      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &      17.3733  &        0.247     &    70.269  &         0.009        &       14.232    &       20.515     \\\\\n",
       "\\textbf{x1}    &       1.0235  &        2.262     &     0.453  &         0.729        &      -27.713    &       29.760     \\\\\n",
       "\\textbf{x2}    &      -0.3081  &       30.092     &    -0.010  &         0.993        &     -382.663    &      382.047     \\\\\n",
       "\\textbf{x3}    &       0.2789  &        1.711     &     0.163  &         0.897        &      -21.465    &       22.023     \\\\\n",
       "\\textbf{x4}    &      -1.0127  &        9.720     &    -0.104  &         0.934        &     -124.519    &      122.494     \\\\\n",
       "\\textbf{x5}    &      -1.0356  &        8.615     &    -0.120  &         0.924        &     -110.497    &      108.426     \\\\\n",
       "\\textbf{x6}    &       0.5976  &        1.295     &     0.461  &         0.725        &      -15.861    &       17.057     \\\\\n",
       "\\textbf{x7}    &       0.2129  &       23.358     &     0.009  &         0.994        &     -296.578    &      297.004     \\\\\n",
       "\\textbf{x8}    &      -0.3877  &       24.867     &    -0.016  &         0.990        &     -316.349    &      315.574     \\\\\n",
       "\\textbf{x9}    &       0.6755  &       25.827     &     0.026  &         0.983        &     -327.485    &      328.836     \\\\\n",
       "\\textbf{x10}   &      -0.1562  &        9.422     &    -0.017  &         0.989        &     -119.876    &      119.563     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 12.359 & \\textbf{  Durbin-Watson:     } &    2.150  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.002 & \\textbf{  Jarque-Bera (JB):  } &    7.063  \\\\\n",
       "\\textbf{Skew:}          &  1.532 & \\textbf{  Prob(JB):          } &   0.0293  \\\\\n",
       "\\textbf{Kurtosis:}      &  5.178 & \\textbf{  Cond. No.          } &     517.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   qsec   R-squared:                       0.979\n",
       "Model:                            OLS   Adj. R-squared:                  0.766\n",
       "Method:                 Least Squares   F-statistic:                     4.609\n",
       "Date:                Thu, 18 Sep 2025   Prob (F-statistic):              0.349\n",
       "Time:                        22:35:08   Log-Likelihood:               -0.25847\n",
       "No. Observations:                  12   AIC:                             22.52\n",
       "Df Residuals:                       1   BIC:                             27.85\n",
       "Df Model:                          10                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         17.3733      0.247     70.269      0.009      14.232      20.515\n",
       "x1             1.0235      2.262      0.453      0.729     -27.713      29.760\n",
       "x2            -0.3081     30.092     -0.010      0.993    -382.663     382.047\n",
       "x3             0.2789      1.711      0.163      0.897     -21.465      22.023\n",
       "x4            -1.0127      9.720     -0.104      0.934    -124.519     122.494\n",
       "x5            -1.0356      8.615     -0.120      0.924    -110.497     108.426\n",
       "x6             0.5976      1.295      0.461      0.725     -15.861      17.057\n",
       "x7             0.2129     23.358      0.009      0.994    -296.578     297.004\n",
       "x8            -0.3877     24.867     -0.016      0.990    -316.349     315.574\n",
       "x9             0.6755     25.827      0.026      0.983    -327.485     328.836\n",
       "x10           -0.1562      9.422     -0.017      0.989    -119.876     119.563\n",
       "==============================================================================\n",
       "Omnibus:                       12.359   Durbin-Watson:                   2.150\n",
       "Prob(Omnibus):                  0.002   Jarque-Bera (JB):                7.063\n",
       "Skew:                           1.532   Prob(JB):                       0.0293\n",
       "Kurtosis:                       5.178   Cond. No.                         517.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ols=sm.add_constant(Xtrain_scaled)\n",
    "ols = sm.OLS(ytrain, X_ols)\n",
    "results = ols.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c172f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 train: 0.978762192523052\n",
      "R2 test: 0.37610389619995843\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = results.predict(sm.add_constant(Xtrain_scaled))\n",
    "y_pred_test  = results.predict(sm.add_constant(Xtest_scaled))\n",
    "\n",
    "print(\"R2 train:\", r2_score(ytrain, y_pred_train))\n",
    "print(\"R2 test:\",  r2_score(ytest, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8934bc9",
   "metadata": {},
   "source": [
    "El modelo ajusta muy bien los datos de entrenamiento (R2 = 0.98), pero su desempe√±o en prueba cae mucho (R2 = 0.38), lo que indica sobreajuste con solo 12 datos de entrenamiento y muchas variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db34c1a",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bcb43fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercepto 17.433333333333334 Coeficientes [-1.91180995 -1.47680791  1.49298425  1.89807932 -0.66063204  1.45742956\n",
      " -0.05266481  1.49531023 -1.5340845  -0.82839949] R2 Train 0.9490586030413806 R2 Test 0.5420583567892368\n"
     ]
    }
   ],
   "source": [
    "# Lambda: .7\n",
    "ridge = Ridge(alpha=.7)  \n",
    "ridge.fit(Xtrain_scaled, ytrain)\n",
    "y_pred_ridge_train = ridge.predict(Xtrain_scaled)\n",
    "y_pred_ridge_test = ridge.predict(Xtest_scaled)\n",
    "\n",
    "\n",
    "r2_train = r2_score(ytrain, y_pred_ridge_train)\n",
    "r2_test = r2_score(ytest, y_pred_ridge_test)\n",
    "\n",
    "\n",
    "print(\"Intercepto\",ridge.intercept_,\"Coeficientes\",ridge.coef_, \"R2 Train\",r2_train,\"R2 Test\",r2_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f59872e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercepto 17.433333333333334 Coeficientes [-1.87361571 -1.39992258  1.30732255  1.81156932 -0.68552688  1.38448781\n",
      " -0.02635773  1.46043045 -1.41053881 -0.79954933] R2 Train 0.9452638793953312 R2 Test 0.5612475485912081\n"
     ]
    }
   ],
   "source": [
    "# Lambda: .8\n",
    "ridge = Ridge(alpha=.8)  \n",
    "ridge.fit(Xtrain_scaled, ytrain)\n",
    "y_pred_ridge_train = ridge.predict(Xtrain_scaled)\n",
    "y_pred_ridge_test = ridge.predict(Xtest_scaled)\n",
    "\n",
    "\n",
    "r2_train = r2_score(ytrain, y_pred_ridge_train)\n",
    "r2_test = r2_score(ytest, y_pred_ridge_test)\n",
    "\n",
    "\n",
    "print(\"Intercepto\",ridge.intercept_,\"Coeficientes\",ridge.coef_, \"R2 Train\",r2_train,\"R2 Test\",r2_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "915f4403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercepto 17.43333333333333 Coeficientes [-2.07112439 -2.90605012  4.04654138  3.04446081 -0.0214042   2.34685586\n",
      " -0.24193943  2.00723997 -3.10258427 -1.39667525] R2 Train 0.9805234796986707 R2 Test 0.23411656981043372\n"
     ]
    }
   ],
   "source": [
    "# Lambda: .1\n",
    "ridge = Ridge(alpha=.1)  \n",
    "ridge.fit(Xtrain_scaled, ytrain)\n",
    "y_pred_ridge_train = ridge.predict(Xtrain_scaled)\n",
    "y_pred_ridge_test = ridge.predict(Xtest_scaled)\n",
    "\n",
    "\n",
    "r2_train = r2_score(ytrain, y_pred_ridge_train)\n",
    "r2_test = r2_score(ytest, y_pred_ridge_test)\n",
    "\n",
    "\n",
    "print(\"Intercepto\",ridge.intercept_,\"Coeficientes\",ridge.coef_, \"R2 Train\",r2_train,\"R2 Test\",r2_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f90358",
   "metadata": {},
   "source": [
    "Lambda = 0.1 (muy baja) los coeficientes son grandes, el modelo ajusta casi perfectamente el entrenamiento (R2 = 0.98), pero falla mucho en test (R2 = 0.23). Esto indica sobreajuste.\n",
    "\n",
    "Lambda = 0.7 (intermedia) los coeficientes se reducen, el ajuste en train baja un poco (R2 = 0.95), pero la predicci√≥n en test mejora significativamente (R2 = 0.54). Aqu√≠ hay un mejor equilibrio entre ajustar y generalizar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae876bce",
   "metadata": {},
   "source": [
    "**_______________________________________________________________________________________________________________________**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bfc058",
   "metadata": {},
   "source": [
    "### Regresi√≥n dummies (y = mpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98ffa10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Motor Trend Car Road Tests.xlsx\")\n",
    "df = df.drop(columns=[\"model\"])\n",
    "X = df.drop(columns=[\"mpg\"])\n",
    "y = df[\"mpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe6f40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.get_dummies(X, columns=['cyl','gear','carb'])\n",
    "X = X.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb841041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01830887 -0.07862013  1.96436269 -2.60198229  0.38957261  2.02955372\n",
      "  0.64184981  0.00967623 -1.82438515  1.81470893 -1.72580159 -1.1309654\n",
      "  2.856767   -1.55689123 -2.61736311 -0.70136616 -0.57998694  0.54005481\n",
      "  4.91555263] 22.056737406563265 R2 0.8976755550941543\n"
     ]
    }
   ],
   "source": [
    "modelo=LinearRegression()\n",
    "modelo.fit(X,y)\n",
    "y_pred= modelo.predict(X)\n",
    "r2=r2_score(y,y_pred)\n",
    "print(modelo.coef_,modelo.intercept_, \"R2\",r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b7a46",
   "metadata": {},
   "source": [
    "Se hizo una regresi√≥n lineal para predecir el consumo de combustible (mpg) usando las caracter√≠sticas del auto y convirtiendo algunas variables en categor√≠as (dummies). El modelo explica casi el 90% de la variaci√≥n en mpg, lo que significa que ajusta bastante bien. El intercepto indica el valor base de mpg, y los coeficientes muestran c√≥mo cada caracter√≠stica afecta el consumo: los positivos aumentan el mpg y los negativos lo reducen. En pocas palabras, el modelo captura bien c√≥mo las distintas propiedades del auto influyen en su rendimiento de combustible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e7fef7",
   "metadata": {},
   "source": [
    "### Train - Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f576900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest=train_test_split(X,y,train_size=.4,random_state=137)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a213baf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 train: 1.0\n",
      "R2 test: 0.28303765950187\n"
     ]
    }
   ],
   "source": [
    "modelo_dummies = LinearRegression()\n",
    "modelo_dummies.fit(Xtrain, ytrain)\n",
    "y_pred_train = modelo_dummies.predict(Xtrain)\n",
    "y_pred_test = modelo_dummies.predict(Xtest)\n",
    "\n",
    "print(\"R2 train:\", r2_score(ytrain, y_pred_train))\n",
    "print(\"R2 test:\",  r2_score(ytest, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b42a6f",
   "metadata": {},
   "source": [
    "El R2 de entrenamiento es 1.0, lo que indica que el modelo ajusta perfectamente los datos de entrenamiento, el de prueba = 0.28 indica sobreajuste. Usar dummies con pocos datos de entrenamiento hace que el modelo se adapte demasiado a esos datos y pierda capacidad de predecir correctamente en nuevas observaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793e20bb",
   "metadata": {},
   "source": [
    "**_______________________________________________________________**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6acce7",
   "metadata": {},
   "source": [
    "### Regresi√≥n Dummies (y = qsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dfc7078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Motor Trend Car Road Tests.xlsx\")\n",
    "df = df.drop(columns=[\"model\"])\n",
    "X = df.drop(columns=[\"qsec\"])\n",
    "y = df[\"qsec\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b5e424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.get_dummies(X, columns=['cyl','gear','carb'])\n",
    "X = X.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eadf9acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01977242  0.00654616 -0.0022363  -0.12178602  0.41294793  0.31242369\n",
      " -1.63412266  1.56234142  0.22363696 -1.78597838 -0.4515632   0.98504616\n",
      " -0.53348296  0.65955613 -0.09818823  0.8623446  -1.00248583 -0.26679561\n",
      " -0.15443106] 16.31671033243791 R2 0.9057905221872551\n"
     ]
    }
   ],
   "source": [
    "modelo=LinearRegression()\n",
    "modelo.fit(X,y)\n",
    "y_pred= modelo.predict(X)\n",
    "r2=r2_score(y,y_pred)\n",
    "print(modelo.coef_,modelo.intercept_, \"R2\",r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b9e5eb",
   "metadata": {},
   "source": [
    "Logra un R2 de 0.91, lo que significa que explica aproximadamente el 90% de la variabilidad de qsec en los datos. Los coeficientes indican c√≥mo cada variable afecta a qsec: valores positivos lo aumentan y valores negativos lo disminuyen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67112b1",
   "metadata": {},
   "source": [
    "### Train - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70952bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest=train_test_split(X,y,train_size=.4,random_state=137)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b22d6ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 train: 1.0\n",
      "R2 test: -0.22029889608534536\n"
     ]
    }
   ],
   "source": [
    "modelo_dummies = LinearRegression()\n",
    "modelo_dummies.fit(Xtrain, ytrain)\n",
    "y_pred_train = modelo_dummies.predict(Xtrain)\n",
    "y_pred_test = modelo_dummies.predict(Xtest)\n",
    "\n",
    "print(\"R2 train:\", r2_score(ytrain, y_pred_train))\n",
    "print(\"R2 test:\",  r2_score(ytest, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c7536",
   "metadata": {},
   "source": [
    "El modelo ajusta perfectamente los datos de entrenamiento (R2 = 1.0), pero no generaliza nada bien a los datos de prueba (R2 negativo, -0.22). Esto indica un sobreajuste extremo, el modelo memoriza los datos de entrenamiento en lugar de aprender patrones generales, por lo que falla completamente al predecir qsec en datos nuevos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7129e24d",
   "metadata": {},
   "source": [
    "### Comparaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f23d96",
   "metadata": {},
   "source": [
    "Para mpg, comparar el modelo sin dummies con el modelo con dummies muestra que incluir las variables categ√≥ricas transforma un poco mejor el ajuste: el R2 sube de aproximadamente 0.87‚Äì0.90 a 0.898. Esto significa que el modelo con dummies explica un poco m√°s la variabilidad de mpg, aunque la mejora no es muy grande porque el dataset es peque√±o y algunas categor√≠as tienen pocas observaciones.\n",
    "\n",
    "Para qsec, la comparaci√≥n es similar: el modelo sin dummies tiene un R2 de aproximadamente 0.875, mientras que con dummies sube a 0.906. Esto indica que agregar las variables categ√≥ricas ayuda a explicar mejor los tiempos de aceleraci√≥n, aunque la diferencia sigue siendo moderada. En ambos casos, usar dummies mejora el modelo, pero no de manera dram√°tica debido al tama√±o y la estructura del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0fd5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
