{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e261b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644b8781",
   "metadata": {},
   "source": [
    "## Regresión Logística simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324c4e7c",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55ba6d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Default.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c97b07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9671\n"
     ]
    }
   ],
   "source": [
    "X = df[[\"balance\", \"income\", \"student\"]].copy()\n",
    "X[\"student\"] = X[\"student\"].map({\"No\": 0, \"Yes\": 1}).astype(float)\n",
    "X[\"balance\"] = X[\"balance\"].astype(float)\n",
    "X[\"income\"] = X[\"income\"].astype(float)\n",
    "\n",
    "y = df[\"default\"].map({\"No\": 0, \"Yes\": 1})\n",
    "\n",
    "Lr = LogisticRegression(max_iter=1000)\n",
    "Lr.fit(X, y)\n",
    "\n",
    "y_pred = Lr.predict(X)\n",
    "\n",
    "accuracy = (y_pred == y).mean()\n",
    "\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbb8adb",
   "metadata": {},
   "source": [
    "### AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ff2d3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9070324073944639\n"
     ]
    }
   ],
   "source": [
    "y_prob = Lr.predict_proba(X)[:, 1]  \n",
    "\n",
    "auc_score = roc_auc_score(y, y_prob)\n",
    "\n",
    "print(\"AUC:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8769f0b7",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc01a3c4",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60511997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michelle Trigo\\AppData\\Local\\Temp\\ipykernel_11248\\3065813103.py:33: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  y_pred_final = mode(y_preds, axis=0).mode[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9846\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Default.csv\")\n",
    "y = df[\"default\"].map({\"No\": 0, \"Yes\": 1})\n",
    "df[\"student\"] = df[\"student\"].map({\"No\": 0, \"Yes\": 1})\n",
    "X = df.drop(columns=[\"default\"])\n",
    "\n",
    "all_features = [\"balance\", \"income\",\"student\"] \n",
    "B = 5000\n",
    "\n",
    "# Matriz para guardar predicciones de clase de cada modelo\n",
    "y_preds = np.zeros((B, len(df)), dtype=int)\n",
    "\n",
    "models = []\n",
    "selected_features_list = []\n",
    "\n",
    "for b in range(B):\n",
    "    # Seleccionar 2 columnas al azar \n",
    "    selected_features = np.random.choice(all_features, size=2, replace=False)\n",
    "    selected_features_list.append(selected_features)\n",
    "    X_b = X[selected_features]\n",
    "    \n",
    "    # Bootstrap de filas\n",
    "    X_b_resampled, y_b = resample(X_b, y, replace=True, n_samples=5000)\n",
    "    \n",
    "    # Entrenar árbol de decisión\n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.fit(X_b_resampled, y_b)\n",
    "    models.append(tree)\n",
    "\n",
    "# Predecir usando cada modelo sobre sus columnas correspondientes\n",
    "for i in range(B):\n",
    "    y_preds[i, :] = models[i].predict(X[selected_features_list[i]])\n",
    "\n",
    "y_pred_final = mode(y_preds, axis=0).mode[0]\n",
    "\n",
    "# Calcular accuracy\n",
    "accuracy = (y_pred_final == y).mean()\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac85c161",
   "metadata": {},
   "source": [
    "## Comparación \n",
    "Con la regresión logística simple, el modelo logró un accuracy de 0.9671. Este modelo es bastante bueno, pero es lineal, es decir, busca una relación directa entre las variables (balance, ingreso y ser estudiante) y la probabilidad de default. Esto puede limitar su capacidad de capturar relaciones más complejas entre los datos.\n",
    "\n",
    "Por otro lado, al usar bagging con árboles de decisión, el accuracy subió a 0.9846. Bagging combina muchos árboles de decisión entrenados con diferentes muestras y características, y luego hace una votación por mayoría. Esto permite al modelo capturar patrones más complejos y reducir errores que un solo árbol o un modelo lineal podría cometer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16303f65",
   "metadata": {},
   "source": [
    "# Conclusión\n",
    "Ambos modelos funcionan muy bien, pero bagging supera a la regresión logística en este caso porque es más flexible y robusto. La mejora en accuracy muestra que, para este conjunto de datos, combinar muchos árboles ayuda a predecir mejor el default que usar un solo modelo lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e8513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
